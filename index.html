<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<script src="https://kit.fontawesome.com/0d3f0e0034.js" crossorigin="anonymous"></script>
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="shortcut icon" href="">
	<link rel="stylesheet" href="jemdoc.css" type="text/css">
	<title>Zhirui Gao - Homepage</title>

	<style>
        /* 主内容区白色背景 */
        #layout-content {
            background-color: white;
            max-width: 900px;
            margin: 5px auto 0;  /* 顶部保留25px间距 */
            padding: 20px;        /* 增加内边距 */
            box-shadow: 0 2px 10px rgba(0,0,0,0.1); /* 可选：添加轻微阴影 */
            border-radius: 4px;    /* 可选：圆角效果 */
        }

        /* 确保body背景不是白色 */
        body {
            background-color: #f5f5f5; /* 浅灰色背景 */
            padding: 0 0;         /* 上下留白 */
        }

        /* 调整表格间距 */
        #layout-content table {
            margin-bottom: 20px;
        }

        /* 响应式调整 */
        @media (max-width: 768px) {
            #layout-content {
                padding: 20px;
                margin: 15px auto;
            }
        }
		h4 a {
		color: #0066cc !important; /* 标准蓝色 */
		text-decoration: none;     /* 去除下划线 */
	}
		h1 a {
		color: #e75e0a !important; /* 标准蓝色 */
		text-decoration: none;     /* 去除下划线 */
	}


    html {
        font-size: 17px; /* 增大全局字体 */
    }


    </style>
	

</head>

<body>

	<!-- <nav class="navbar navbar-dark navbar-expand-lg fixed-top">
		<div id="layout-menu">
			<a href="#publication">Publications</a>
			<a href="#experience">Experiences</a>
			<a href="#award">Awards</a>
		</div>
	</nav> -->

	<div id="layout-content" style="margin-top:25px">
		<table>
			<tbody>
				<tr>
					<!-- Identity -->
					<td width="100%">
						<div id="toptitle">
							<h1>
								<a href="https://github.com/zhirui-gao">Zhirui Gao </a> 
							</h1>
						<!-- </div> -->
						<h3>Ph.D. Student</h3>
						<p>
							School of Computer Science and Technology<br>
							National University of Defense Technology (NUDT)<br>
							ChangSha, People's Republic of China<br>
							<br>
							Email: <u><code>gzrer2018 [at] gmail.com</code></u> 
						</p>
						<h4>
							<a href="./assets/zhirui_CV.pdf">
								<span class="icon"><i class="fa-solid fa-file-lines"></i>[Curriculum Vitae]</span>
							</a>
							&nbsp;
							<a href="https://scholar.google.com/citations?user=IqtwGzYAAAAJ&hl=zh-CN">
								<span class="icon"><i class="fa-solid fa-graduation-cap"></i>[Google Scholar]</span>
							</a>
							&nbsp;
							<a href="https://github.com/zhirui-gao">
								<span class="icon"><i class="fa-brands fa-github"></i>[GitHub]</span>
							</a>
							&nbsp;
							<a href="https://zhirui-gao.github.io/wechat.html">
								<span class="icon"><i class="fa-brands fa-weixin"></i>[WeChat]</span>
							</a>
						</h4>
					</td>
					<!-- Portrait -->
					<td width="100%">
						<img src="assets/portrait.png" height="250">
					</td>
				<tr>
			</tbody>
		</table>

		<div id="biography">
			<h2>👤 Biography</h2>
			<p style="text-align:justify">
				Hi there👋, my name is Zhirui Gao (in Chinese: 高志锐).
				I am a third-year Ph.D. student  of
				<b>National University of Defense Technology</b>, supervised by
				<a href="https://kevinkaixu.net/">Prof. Kai Xu</a>, Prof. Wei Chen and <a href="https://renjiaoyi.github.io/">Assoc.Prof. Renjiao Yi</a>.
				From 2017 to 2021, I earned my Bachelor's degree from China University of Geosciences. <b>I am interested in 3D vision and graphics，
					 including 3D reconstruction, neural rendering (NeRF/3DGS), and AIGC.</b>
			
			<br><br>
				I'm always open to research discussions and collaborations. Feel free to 📲contact me if you are interested.
			
			</p>
		</div><br>

		<div id="news">
			<h2>📢 News</h2>
			<ul>
				<li>
					[2025-07] Our paper <a href="https://lanlan96.github.io/BoxFusion/">BoxFusion</a> is available on arxiv.  
					
				</li>

				<li>
					[2025-06] Two papers are accepted by ICCV2025!
					(<a href="https://zhirui-gao.github.io/PartGS/">PartGS</a> and <a href="https://zhirui-gao.github.io/CurveGaussian/">CurveGaussian</a>).    
				</li>	
				
				<li>
					[2025-02] Our paper <a href="https://zhirui-gao.github.io/PoseProbe.github.io/">PoseProbe</a> is accepted by IEEE TSCVT.  
				</li>

				<li>
					[2023-12] Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10446550/">Fdc-nerf</a> is accepted by ICASSP (Oral), 
					thanks for <a href="https://gaohchen.github.io/">Huachen</a>.  
				</li>

				<li>
					[2023-06] Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.html">2d3d-matr</a> is accepted by ICCV2023!  
				</li>
				
				

				<li>
					[2023-04] Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10168686/">Delving Into Crispness</a> is accepted by TIP!  
				</li>

				<li>
					[2023-03] Our paper <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.html">NEF</a> is accepted by CVPR2023!  
				</li>

				<li>
					[2023-01] Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10897650/">Deep Template Matching</a> is accepted by IEEE CVMJ! 
				</li>
			</ul>
		</div><br>

		<div id="publication">
			<h2>📑 Selected Publications [<a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=IqtwGzYAAAAJ&sortby=pubdate">Google Scholar</a>]</h2>
			<!-- <p>*: equal contribution; †: project lead</p> -->

			<table>
				<tbody>
					<tr style="background-color: lightyellow;">
						<td style="padding:0.5%; width:25%; vertical-align:middle">
							<div class="badge">arXiv 2025</div>
							<img src="teasers/boxfusion.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:100%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/abs/2506.15610">
										BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion
									</a><br>
									Yuqing Lan, Chenyang Zhu, <b>Zhirui Gao</b>, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu<br>
									<p style="margin-top:3px">
										[<a href="https://arxiv.org/abs/2506.15610">arXiv</a>]
										[<a href="https://lanlan96.github.io/BoxFusion/">Project Page</a>]
										[<a href="https://github.com/lanlan96/BoxFusion">Code</a>]
									</p>
								</li>
									We propose a  novel reconstruction-free paradigm for online open-vocabulary 3D object detection,
									 which models structural object layouts with desirable running and memory efficiency.
								
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:0.5%; width:25%; vertical-align:middle">
							<div class="badge">ICCV 2025</div>
								<img src="teasers/curvegs.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:100%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/abs/2506.21401">
										Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction
									</a><br>
									<b>Zhirui Gao</b>, Renjiao Yi, Yaqiao Dai, Xuening Zhu, Wei Chen, Chenyang Zhu, Kai Xu 
									<p style="margin-top:3px">
										<p style="margin-top:3px">
										[<a href="https://arxiv.org/abs/2506.21401">arXiv</a>]
										[<a href="https://zhirui-gao.github.io/CurveGaussian/">Project Page</a>]
										[<a href="https://github.com/zhirui-gao/Curve-Gaussian">Code</a>]
									</p>
									</p>
								</li>
									We present an end-to-end method for reconstructing 3D parametric curves directly from multi-view edge maps. 
									Contrasting with the existing reconstruct-and-fit pipelines, our one-stage approach optimizes 3D parametric curves directly from 2D edge maps.
							</ul>
						</td>
					</tr>
					
						<tr style="background-color: lightyellow;">
						<td style="width:25%; vertical-align:middle">
							<div class="badge">ICCV 2025</div>
							<img src="teasers/partgs.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/abs/2408.10789">
										Self-supervised Learning of Hybrid Part-aware 3D Representations of 2D Gaussians and Superquadrics
									</a><br>
									<b>Zhirui Gao</b>, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu
									<p style="margin-top:3px">
										[<a href="https://arxiv.org/abs/2408.10789">arXiv</a>]
										[<a href="https://zhirui-gao.github.io/PartGS/">Project Page</a>]
										[<a href="https://github.com/zhirui-gao/PartGS">Code</a>]
									</p>
								</li>
									We introduce PartGS, a self-supervised part-aware reconstruction framework that integrates 
									2D Gaussians and superquadrics to parse objects and scenes into an interpretable decomposition, 
									leveraging multi-view image inputs to uncover 3D structural information..
								
							</ul>
						</td>
					</tr>

					<tr>
						<td style="padding:0.5%; width:25%; vertical-align:middle">
							<div class="badge">IEEE TCSVT 2025</div>
								<img src="teasers/poseprobe.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:100%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2408.16690">
										Generic Objects as Pose Probes for Few-shot View Synthesis
									</a><br>
									<b>Zhirui Gao</b>, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu
									<p style="margin-top:3px">
										<p style="margin-top:3px">
										[<a href="https://arxiv.org/pdf/2408.16690">arXiv</a>]
										[<a href="https://zhirui-gao.github.io/PoseProbe.github.io/">Project Page</a>]
										[<a href="https://github.com/zhirui-gao/PoseProbe">Code</a>]
									</p>
									</p>
								</li>
									We propose to utilize everyday objects, commonly found in both images and real life, 
									as "pose probes" to tackle few-view (3~6 unposed images) NeRF reconstruction.
							</ul>
						</td>
					</tr>
					
						<tr style="background-color: lightyellow;">
						<td style="width:25%; vertical-align:middle">
							<div class="badge">ICCV 2023</div>
							<img src="teasers/li_iccv23.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2308.05667">
										2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds
									</a><br>
									Minhao Li, Zheng Qin, <b>Zhirui Gao</b>, Renjiao Yi, Chenyang Zhu, Yulan Guo, Kai Xu
									<p style="margin-top:3px">
										[<a href="https://arxiv.org/pdf/2308.05667">arXiv</a>]
										[<a href="https://github.com/minhaolee/2D3DMATR">Project Page</a>]
										[<a href="https://github.com/minhaolee/2D3DMATR">Code</a>]
									</p>
								</li>
									The commonly adopted detect-then-match approach to registration finds difficulties in the cross-modality cases due to the 
									incompatible keypoint detection and inconsistent feature description. We propose, 2D3D-MATR, a detection-free method for 
									accurate and robust registration between images and point clouds.
							</ul>
						</td>
					</tr>
					

					<tr>
						<td style="padding:0.5%; width:25%; vertical-align:middle">
							<div class="badge">CVPR 2023</div>
								<img src="teasers/ye_cvpr23.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:100%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/abs/2303.07653">
										NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images
									</a><br>
									Yunfan Ye, Renjiao Yi, <b>Zhirui Gao</b>, Chenyang Zhu, Zhiping Cai, Kai Xu
									<p style="margin-top:3px">
										<p style="margin-top:3px">
										[<a href="https://arxiv.org/abs/2303.07653">arXiv</a>]
										[<a href="https://yunfan1202.github.io/NEF/">Project Page</a>]
										[<a href="https://github.com/yunfan1202/NEF_code">Code</a>]
									</p>
									</p>
								</li>
									We study the problem of reconstructing 3D feature curves of an object from a set of calibrated multi-view images. 
									To do so, we learn a neural implicit field representing the density distribution of 3D edges which we refer to as Neural Edge Field (NEF). 
									Inspired by NeRF, NEF is optimized with a view-based rendering loss 
									where a 2D edge map is rendered at a given view and is compared to the ground-truth edge map extracted from the image of that view.
							</ul>
						</td>
					</tr>


						<tr style="background-color: lightyellow;">
						<td style="width:25%; vertical-align:middle">
							<div class="badge">CVMJ 2024</div>
							<img src="teasers/gao_cvm24.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2303.08438">
										Learning Accurate Template Matching with Differentiable Coarse-to-Fine Correspondence Refinement
									</a><br>
									<b>Zhirui Gao</b>, Renjiao Yi,  Zheng Qin, Yunfan Ye, Chenyang Zhu, Kai Xu
									<p style="margin-top:3px">
										[<a href="https://arxiv.org/pdf/2303.08438">arXiv</a>]
										[<a href="https://github.com/zhirui-gao/Deep-Template-Matching">Project Page</a>]
										[<a href="https://github.com/zhirui-gao/Deep-Template-Matching">Code</a>]
									</p>
								</li>
									Template matching is a fundamental task in computer vision and has been studied for decades. 
									We propose an accurate template matching method based on differentiable coarse-to-fine correspondence refinement. 
									We use an edge-aware module to overcome the domain gap between the mask template and the grayscale image, allowing robust matching.
								
							</ul>
						</td>
					</tr>

					<tr>
						<td style="padding:0.5%; width:25%; vertical-align:middle">
							<div class="badge">IEEE TIP 2023</div>
								<img src="teasers/ye_tip23.png" style="width: 110%;"></img>
						</td>
						<td style="padding:0.5%; padding-left:0px; padding-right:0px; width:100%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2306.15172">
										Delving into Crispness: Guided Label Refinement for Crisp Edge Detection
									</a><br>
									Yunfan Ye, Renjiao Yi, <b>Zhirui Gao</b>, Zhiping Cai, Kai Xu
									<p style="margin-top:3px">
										<p style="margin-top:3px">
										[<a href="https://arxiv.org/pdf/2306.15172">arXiv</a>]
										[<a href="https://github.com/yunfan1202/Delving-into-Crispness">Project Page</a>]
										[<a href="https://github.com/yunfan1202/Delving-into-Crispness">Code</a>]
									</p>
									</p>
								</li>
									We find that label quality is more important than model design to achieving crisp edge detection. 
									We propose an iterative Canny-guided refinement of human-labeled edges whose result can be used to train crisp edge detectors.
							</ul>
						</td>
					</tr>

				</tbody>
			</table>

		</div><br>



		<div id="education">
			<h2>🎓 Educations</h2>
			<ul>

				<li>
					<div style="float:left; text-align:left">
						<b>Ph.D. student</b>, College of Computer Science,
						<b>National University of Defense Technology</b>
					</div>
					<div style="float:right; text-align:right">2023 - present</div><br>
				</li>

				<li>
					<div style="float:left; text-align:left">
						<b>Master</b>, College of Computer Science,
						<b>National University of Defense Technology</b>
					</div>
					<div style="float:right; text-align:right">2021 - 2022</div><br>
				</li>
				<li>
					<div style="float:left; text-align:left">
						<b>B.Eng.</b>, College of Computer Science,
						<b>China University of Geosciences</b>
					</div>
					<div style="float:right; text-align:right">2017 - 2021</div><br>
					<ul>
						<li>GPA: 4.08/5.00; Average Score: 91.5/100</li>
						<li>Comprehensive Ranking: <b>1/151</b></li>
					</ul>
				</li>
			</ul>
		</div><br>

		<div id="award">
			<h2>🏆 Honors & Awards</h2>
			<ul>


				<li>
					<div style="float:left; text-align:left">Second Prize Scholarship </div>
					<div style="float:right; text-align:right">2024</div>
				</li>

				<li>
					<div style="float:left; text-align:left">First Prize Freshman Scholarship </div>
					<div style="float:right; text-align:right">2023</div>
				</li>

			
				<li>
					<div style="float:left; text-align:left">Outstanding Graduate of China University of Geosciences, Top 5%</div>
					<div style="float:right; text-align:right">2021</div>
				</li>

					<li>
					<div style="float:left; text-align:left">Outstanding Degree Thesis of China University of Geosciences, Top 5%</div>
					<div style="float:right; text-align:right">2021</div>
				</li>

				<li>
					<div style="float:left; text-align:left">National Encouragement Scholarship, Top 5%</div>
					<div style="float:right; text-align:right">2019,2020</div>
				</li>

				<li>
					<div style="float:left; text-align:left"><b>Silver Medal of ACM-ICPC</b> </div>
					<div style="float:right; text-align:right">2019</div>
				</li>

				<li>
					<div style="float:left; text-align:left"><b>National Scholarship, Top 2%</b>, 
						Ministry of Education (the highest honor scholarship in China)</div>
					<div style="float:right; text-align:right">2018</div>
				</li>
				
			</ul>
		</div><br>

		<div id="service">
			<h2>📝 Services</h2>
			<ul>
				<li>
					Conference Reviewer: ICCV2025， MM2025
				</li>
				<li>
					Journal Reviewer: TCSVT
				</li>
			</ul>
		</div><br>

		<div id="footer">
			<div id="footer-text"></div>
		</div>

		<p style="text-align:center">
			© Zhirui Gao
			<script type="text/javascript" language="javascript">
				if (Date.parse(document.lastModified) != 0) {
					document.write(" | Last updated: " + document.lastModified.substr(0, 10));
				}
			</script>
		</p>

	</div>

</body>

</html>
