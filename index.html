<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<script src="https://kit.fontawesome.com/0d3f0e0034.js" crossorigin="anonymous"></script>
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="shortcut icon" href="">
	<link rel="stylesheet" href="jemdoc.css" type="text/css">
	<title>Chenguo Lin - Homepage</title>
</head>

<body>

	<!-- <nav class="navbar navbar-dark navbar-expand-lg fixed-top">
		<div id="layout-menu">
			<a href="#publication">Publications</a>
			<a href="#experience">Experiences</a>
			<a href="#award">Awards</a>
		</div>
	</nav> -->

	<div id="layout-content" style="margin-top:25px">
		<table>
			<tbody>
				<tr>
					<!-- Identity -->
					<td width="100%">
						<div id="toptitle">
							<h1>
								<a href="http://chenguolin.github.io">Chenguo Lin</a>
							</h1>
						</div>
						<h3>Ph.D. Student</h3>
						<p>
							Wangxuan Institute of Computer Technology (WICT)<br>
							School of Intelligence Science and Technology<br>
							Peking University<br>
							Beijing, People's Republic of China<br>
							<br>
							Email: <u><code>chenguolin[at]stu.pku.edu.cn</code></u> <!-- or <u><code>linchenguo0622 [at] gmail.com</code></u> -->
						</p>
						<h4>
							<!-- <a href="ChenguoLIN_CV.pdf">
								<span class="icon"><i class="fa-solid fa-file-lines"></i>[Curriculum Vitae]</span>
							</a>
							&nbsp; -->
							<a href="https://scholar.google.com/citations?user=jZ7MDcMAAAAJ">
								<span class="icon"><i class="fa-solid fa-graduation-cap"></i>[Google Scholar]</span>
							</a>
							&nbsp;
							<a href="http://github.com/chenguolin">
								<span class="icon"><i class="fa-brands fa-github"></i>[GitHub]</span>
							</a>
							&nbsp;
							<a href="https://twitter.com/lin_chenguo">
								<span class="icon"><i class="fa-brands fa-x-twitter"></i>[Twitter]</span>
							</a>
							&nbsp;
							<a href="https://chenguolin.github.io/wechat.html">
								<span class="icon"><i class="fa-brands fa-weixin"></i>[WeChat]</span>
							</a>
							<!-- &nbsp;
							<a href="https://www.linkedin.com/in/chenguo-lin-829449200">
								<span class="icon"><i class="fa-brands fa-linkedin"></i>[LinkedIn]</span>
							</a> -->
						</h4>
					</td>
					<!-- Portrait -->
					<td width="100%">
						<img src="assets/portrait.jpg" height="250">
					</td>
				<tr>
			</tbody>
		</table>

		<div id="biography">
			<h2>üë§ Biography</h2>
			<p style="text-align:justify">
				Hi thereüëã, my name is Chenguo Lin (in Chinese: ÊûóÁêõÊûú).
				I am a third-year Ph.D. student of computer science and artificial intelligence at
				<a href="https://english.pku.edu.cn">Peking University</a>, China, supervised by
				<a href="https://scholar.google.com/citations?user=Fqqx4HsAAAAJ">Prof. Yadong Mu</a>.
				Currently, I'm working as a research intern at <a href="https://seed.bytedance.com/en">ByteDance</a>,
				focusing on Video/3D/4D AIGC and Spatial Intelligence.
				I've spent wonderful time as a research assistant/intern with
				<a href="https://www.microsoft.com/en-us/research/people/wuzhiron">Dr. Zhirong Wu</a> at
				<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia">Microsoft Research Asia (MSRA)</a>,
				<a href="http://luoping.me">Prof. Ping Luo</a> at <a href="https://www.hku.hk">HKU</a>, and
				<a href="https://chaoningzhang.github.io">Dr. Chaoning Zhang</a> at <a href="https://www.kaist.ac.kr/en">KAIST</a>.
				</br></br>
				My research focuses on advancing the frontier of <b>Multi-modal World Models</b>: AI systems to üëÄperceive, üß†reason
				about, and ü¶æact within the physical world. I pursue this vision through three tightly connected directions:
				(1) <b>Generative Models for Controllable Video/3D/4D Content</b>,
				(2) <b>Multi-modality Learning for Visual Reasoning</b>, and
				(3) <b>Geometry-grounded Perception for Spatial Intelligence</b>.
				By integrating these three directions, my overarching goal is to develop general-purpose intelligence systems:
				AI that can see, understand, generate, and interact with complex physical environments.
				</br></br>
				I'm always open to research discussions and collaborations. Feel free to üì≤contact me if you are interested.
			</p>
		</div><br>

		<div id="news">
			<h2>üì¢ News</h2>
			<ul>
				<li>
					[2025-08] A unified 2D & 3D instruction-to-layout generation framework
					(<a href="https://arxiv.org/abs/2407.07580">InstructLayout</a>) was accepted to <b>T-PAMI</b> 2025.
				</li>
				<li>
					[2025-07] We released a feed-forward model for 4D content creation from monocular videos:
					<a href="https://chenguolin.github.io/projects/MoVieS">MoVieS</a>
					<a href="https://github.com/chenguolin/MoVieS" target="_blank">
						<img src="https://img.shields.io/github/stars/chenguolin/MoVieS?style=social"
							alt="MoVieS GitHub stars" style="vertical-align: text-bottom;" />
					</a>.
				</li>
				<li>
					[2025-06] We released a 3D-native DiT that generates 3D objects in parts:
					<a href="https://wgsxm.github.io/projects/partcrafter">PartCrafter</a>
					<a href="https://github.com/wgsxm/PartCrafter" target="_blank">
						<img src="https://img.shields.io/github/stars/wgsxm/PartCrafter?style=social"
							alt="PartCrafter GitHub stars" style="vertical-align: text-bottom;" />
					</a>.
				</li>
				<li>
					[2025-01] Two papers about 3D object & dynamics generation
					(<a href="https://chenguolin.github.io/projects/DiffSplat">DiffSplat</a> &
					<a href="https://wgsxm.github.io/projects/omniphysgs">OmniPhysGS</a>)
					were accepted to <b>ICLR</b> 2025.
				</li>
				<li>
					[2024-09] One paper about generalizable single-view human reconstruction
					(<a href="https://humansplat.github.io">HumanSplat</a>) was accepted to <b>NeurIPS</b> 2024.
				</li>
				<li>
					[2024-07] One paper about large-scale time-series pretraining
					(<a href="https://github.com/chenguolin/NuTime">NuTime</a>) was accepted to <b>TMLR</b> 2024.
				</li>
				<li>
					[2024-01] One paper about 3D scene synthesis
					(<a href="https://chenguolin.github.io/projects/InstructScene">InstructScene</a>) was accepted to <b>ICLR</b> 2024
					as a <b style="color: #d00000;">spotlight</b> paper.
				</li>
			</ul>
		</div><br>

		<div id="publication">
			<h2>üìë Selected Publications [<a href="https://scholar.google.com/citations?user=jZ7MDcMAAAAJ">Google Scholar</a>]</h2>
			<p>*: equal contribution; ‚Ä†: project lead</p>

			<table>
				<tbody>
					<tr style="background-color: lightyellow;">
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">arXiv 2025</div>
							<video src="teasers/movies.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2507.10065">
										MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second
									</a><br>
									<u><b>Chenguo Lin</b></u>*, Yuchen Lin*, Panwang Pan‚Ä†, Yifan Yu, Honglei Yan,<br>
									Katerina Fragkiadaki, Yadong Mu<br>
									<em>Preprint</em> (<b>arXiv</b>), 2025<br>
									<p style="margin-top:3px">
										<!-- [<a href="https://openreview.net/forum?id=">OpenReview</a>] -->
										[<a href="https://arxiv.org/abs/2507.10065">arXiv</a>]
										[<a href="https://chenguolin.github.io/projects/MoVieS">Project Page</a>]
										[<a href="https://github.com/chenguolin/MoVieS">Code</a>]
										<a href="https://github.com/chenguolin/MoVieS" target="_blank">
											<img src="https://img.shields.io/github/stars/chenguolin/MoVieS?style=social"
												alt="MoVieS GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://chenguolin.github.io/projects/MoVieS">MoVieS</a>
									is a feed-forward framework that jointly reconstructs appearance, geometry
									and motion for 4D scene perception from monocular videos.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">arXiv 2025</div>
							<video src="teasers/diff4splat.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2507">
										Diff4Splat: Generalizable 4D Scene Generation with Latent Dynamic Reconstruction Models
									</a><br>
									Panwang Pan*, <u><b>Chenguo Lin</b></u>*, Jingjing Zhao, Chenxin Li, Yuchen Lin,<br>
									Kairun Wen, Yunlong Lin, Yixuan Yuan, Yadong Mu, Zhiwen Fan<br>
									<em>Preprint</em> (<b>arXiv</b>), 2025<br>
									<p style="margin-top:3px">
										<!-- [<a href="https://openreview.net/forum?id=">OpenReview</a>] -->
										<!-- [<a href="https://arxiv.org/abs/2507">arXiv</a>] -->
										<!-- [<a href="https://paulpanwang.github.io/Diff4Splat">Project Page</a>] -->
										<!-- [<a href="https://github.com/paulpanwang/Diff4Splat">Code</a>] -->
										<!-- <a href="https://github.com/paulpanwang/Diff4Splat" target="_blank">
											<img src="https://img.shields.io/github/stars/paulpanwang/Diff4Splat?style=social"
												alt="Diff4Splat GitHub stars" style="vertical-align: text-bottom;"/>
										</a> -->
									</p>
								</li>
								<li>
									TL;DR: <a href="https://">Diff4Splat</a>
									is a generalizable framework for controllable 4D scene generation from a single image using a video diffusion model.
								</li>
							</ul>
						</td>
					</tr>
					<tr style="background-color: lightyellow;">
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">arXiv 2025</div>
							<img src="teasers/partcrafter.png" style="width: 100%;"></img>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2506.05573">
										PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers
									</a><br>
									Yuchen Lin*, <u><b>Chenguo Lin</b></u>*, Panwang Pan‚Ä†, Honglei Yan, Yiqiang Feng,<br>
									Yadong Mu, Katerina Fragkiadaki<br>
									<em>Preprint</em> (<b>arXiv</b>), 2025<br>
									<b style="color: #d00000;">Received more than 1.8K GitHub stars within a month</b><br>
									<p style="margin-top:3px">
										<!-- [<a href="https://openreview.net/forum?id=">OpenReview</a>] -->
										[<a href="https://arxiv.org/abs/2506.05573">arXiv</a>]
										[<a href="https://wgsxm.github.io/projects/partcrafter">Project Page</a>]
										[<a href="https://github.com/wgsxm/PartCrafter">Code</a>]
										<a href="https://github.com/wgsxm/PartCrafter" target="_blank">
											<img src="https://img.shields.io/github/stars/wgsxm/PartCrafter?style=social"
												alt="PartCrafter GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://wgsxm.github.io/projects/partcrafter">PartCrafter</a>
									is a structured 3D generative model that jointly generates multiple parts
									and objects from a single RGB image in one shot.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">arXiv 2025</div>
							<video src="teasers/dynamicverse.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/">
										DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds
									</a><br>
									Kairun Wen*, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, Jian Zhang, Junbin Lu,
									<u><b>Chenguo Lin</b></u>, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan<br>
									<em>Preprint</em> (<b>arXiv</b>), 2025<br>
									<p style="margin-top:3px">
										<!-- [<a href="https://openreview.net/forum?id=">OpenReview</a>] -->
										[<a href="https://arxiv.org/abs/">arXiv</a>]
										[<a href="https://dynamic-verse.github.io/">Project Page</a>]
										[<a href="https://github.com/kairunwen/DynamicVerse">Code</a>]
										<!-- <a href="https://github.com/kairunwen/DynamicVerse" target="_blank">
											<img src="https://img.shields.io/github/stars/kairunwen/DynamicVerse?style=social"
												alt="DynamicVerse GitHub stars" style="vertical-align: text-bottom;"/>
										</a> -->
									</p>
								</li>
								<li>
									TL;DR: <a href="https://github.com/kairunwen/DynamicVerse">DynamicVerse</a>
									is a large-scale multi-modal framework that integrates
									foundation models to convert videos into 4D representations,
									such as geometry, motion, semantics, etc.
								</li>
							</ul>
						</td>
					</tr>
					<tr style="background-color: lightyellow;">
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">ICLR 2025</div>
							<video src="teasers/diffsplat.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2501.16764">
										DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation
									</a><br>
									<u><b>Chenguo Lin</b></u>, Panwang Pan‚Ä†, Bangbang Yang, Zeming Li, Yadong Mu<br>
									<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2025<br>
									<p style="margin-top:3px">
										[<a href="https://openreview.net/forum?id=eajZpoQkGK">OpenReview</a>]
										[<a href="https://arxiv.org/abs/2501.16764">arXiv</a>]
										[<a href="https://chenguolin.github.io/projects/DiffSplat">Project Page</a>]
										[<a href="https://github.com/chenguolin/DiffSplat">Code</a>]
										<a href="https://github.com/chenguolin/DiffSplat" target="_blank">
											<img src="https://img.shields.io/github/stars/chenguolin/DiffSplat?style=social"
												alt="DiffSplat GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://chenguolin.github.io/projects/DiffSplat">DiffSplat</a>
									directly generates 3D Gaussians by taming large text-to-image diffusion models
									from text prompts and single-view images in 1~2 seconds.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">ICLR 2025</div>
							<video src="teasers/omniphysgs.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2501.18982">
										OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation
									</a><br>
									Yuchen Lin, <u><b>Chenguo Lin</b></u>‚Ä†, Jianjin Xu, Yadong Mu<br>
									<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2025<br>
									<p style="margin-top:3px">
										[<a href="https://openreview.net/forum?id=9HZtP6I5lv">OpenReview</a>]
										[<a href="https://arxiv.org/abs/2501.18982">arXiv</a>]
										[<a href="https://wgsxm.github.io/projects/omniphysgs">Project Page</a>]
										[<a href="https://github.com/wgsxm/omniphysgs">Code</a>]
										<a href="https://github.com/wgsxm/omniphysgs" target="_blank">
											<img src="https://img.shields.io/github/stars/wgsxm/omniphysgs?style=social"
												alt="OmniPhysGS GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://wgsxm.github.io/projects/omniphysgs">OmniPhysGS</a>
									synthesizes general physics-based 3D dynamic scenes, and
									can automatically and flexibly model various materials with domain-expert constitutive models.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">NeurIPS 2024</div>
							<video src="teasers/humansplat.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2406.12459">
										HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors
									</a><br>
									Panwang Pan*, Zhuo Su*‚Ä†, <u><b>Chenguo Lin</b></u>*, Zhen Fan, Yongjie Zhang, Zeming Li,<br>
									Tingting Shen, Yadong Mu, Yebin Liu<br>
									<em>Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2024
									<p style="margin-top:3px">
										[<a href="https://openreview.net/forum?id=JBAUg7o8Yv">OpenReview</a>]
										[<a href="https://arxiv.org/abs/2406.12459">arXiv</a>]
										[<a href="https://humansplat.github.io">Project Page</a>]
										[<a href="https://github.com/humansplat/humansplat">Code</a>]
										<a href="https://github.com/humansplat/humansplat" target="_blank">
											<img src="https://img.shields.io/github/stars/humansplat/humansplat?style=social"
												alt="HumanSplat GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://humansplat.github.io">HumanSplat</a>
									digitalizes any human from a single input image in seconds via multi-view image diffusion models
									and latent Gaussian splatting reconstruction.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">T-PAMI 2025</div>
							<img src="teasers/instructlayout.png" style="width: 100%;"></img>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2407.07580">
										InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with Semantic Graph Prior
									</a><br>
									<u><b>Chenguo Lin</b></u>*, Yuchen Lin*, Panwang Pan, Xuanyang Zhang, Yadong Mu<br>
									<em>Transactions on Pattern Analysis and Machine Intelligence</em> (<b>T-PAMI</b>)
									<p style="margin-top:3px">
										[<a href="https://ieeexplore.ieee.org/Xplore/home.jsp">IEEE Xplore</a>]
										[<a href="https://arxiv.org/abs/2407.07580">arXiv</a>]
										[<a href="https://chenguolin.github.io/projects/InstructScene">Project Page</a>]
										[<a href="https://github.com/chenguolin/InstructScene">Code</a>]
									</p>
								</li>
								<li>
									TL;DR: <a href="https://arxiv.org/abs/2407.07580">InstructLayout</a> is an extension of
									<a href="https://chenguolin.github.io/projects/InstructScene">InstructScene</a> that improves
									controllability and fidelity for the layout synthesis of both 2D E-commerce posters and 3D indoor scenes.
								</li>
							</ul>
						</td>
					</tr>
					<tr style="background-color: lightyellow;">
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">ICLR 2024</div>
							<video src="teasers/instructscene.mp4" style="width: 100%;" autoplay loop muted></video>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2402.04717">
										InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior
									</a><br>
									<u><b>Chenguo Lin</b></u>, Yadong Mu<br>
									<em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2024<br>
									<b style="color: #d00000;">Spotlight (acceptance rate: 5%)</b><br>
									<p style="margin-top:3px">
										[<a href="https://openreview.net/forum?id=LtuRgL03pI">OpenReview</a>]
										[<a href="https://arxiv.org/abs/2402.04717">arXiv</a>]
										[<a href="https://chenguolin.github.io/projects/InstructScene">Project Page</a>]
										[<a href="https://github.com/chenguolin/InstructScene">Code</a>]
										<a href="https://github.com/chenguolin/InstructScene" target="_blank">
											<img src="https://img.shields.io/github/stars/chenguolin/InstructScene?style=social"
												alt="InstructScene GitHub stars" style="vertical-align: text-bottom;"/>
										</a>
									</p>
								</li>
								<li>
									TL;DR: <a href="https://chenguolin.github.io/projects/InstructScene">InstructScene</a>
									is a generative framework to synthesize 3D indoor scenes from textual instructions,
									and is composed of a semantic graph prior and a layout decoder.
								</li>
							</ul>
						</td>
					</tr>
					<tr>
						<td style="padding:2.5%; width:25%; vertical-align:middle">
							<div class="badge">TMLR 2024</div>
							<img src="teasers/nutime.png" style="width: 100%;"></img>
						</td>
						<td style="padding:2.5%; padding-left:0px; padding-right:0px; width:75%; vertical-align:top">
							<ul>
								<li>
									<a href="https://arxiv.org/pdf/2310.07402">
										NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time-Series Pretraining
									</a><br>
									<u><b>Chenguo Lin</b></u>*, Xumeng Wen*, Wei Cao, Congrui Huang,<br>
									Jiang Bian, Stephen Lin, Zhirong Wu‚Ä†<br>
									<em>Transactions on Machine Learning Research</em> (<b>TMLR</b>), 2024<br>
									<p style="margin-top:3px">
										[<a href="https://openreview.net/forum?id=TwiSBZ0p9u">OpenReview</a>]
										[<a href="https://arxiv.org/abs/2310.07402">arXiv</a>]
										[<a href="https://github.com/chenguolin/NuTime">Code</a>]
									</p>
								</li>
								<li>
									TL;DR: <a href="https://github.com/chenguolin/NuTime">NuTime</a> is a Transformer-based architecture
									that can take raw values of time-series data as input without any data normalization and transformation.
								</li>
							</ul>
						</td>
					</tr>
				</tbody>
			</table>

		</div><br>

		<div id="experience">
			<h2>üíº Experiences</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">
						<a href="https://seed.bytedance.com/en">ByteDance</a></div>
					<div style="float:right; text-align:right">December 2023 - present</div><br>
					Research Intern<br>
					<!-- Collaborators: <a href="https://paulpanwang.github.io">Panwang Pan</a>,
					<a href="https://www.zemingli.com">Zeming Li</a>,
					<a href="https://ybbbbt.com">Bangbang Yang</a>, <a href="https://suzhuo.github.io">Zhuo Su</a>,
					<a href="https://scholar.google.com/citations?user=S2OksN4AAAAJ">Yifan Yu</a><br> -->
					Topic: Video/3D/4D AIGC, Spatial Intelligence
				</li>
				<li>
					<div style="float:left; text-align:left">
						<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia">Microsoft Research Asia (MSRA)</a></div>
					<div style="float:right; text-align:right">November 2021 - September 2022</div><br>
					Research Intern<br>
					Mentor: <a href="https://scholar.google.com/citations?user=lH4zgcIAAAAJ">Zhirong Wu</a>,
					<a href="https://scholar.google.com/citations?user=c3PYmxUAAAAJ">Stephen Lin</a><br>
					Topic: Self-supervised Representation Learning
				</li>
				<li>
					<div style="float:left; text-align:left">
						<a href="https://www.hku.hk">The University of Hong Kong (HKU)</a></div>
					<div style="float:right; text-align:right">June 2021 - August 2021</div><br>
					Research Assistant<br>
					Advisors: <a href="http://luoping.me">Ping Luo</a>, <a href="https://dingmyu.github.io">Mingyu Ding</a><br>
					Topic: Neural Architecture Search
				</li>
				<li>
					<div style="float:left; text-align:left">
						<a href="https://www.kaist.ac.kr/en">Korea Advanced Institute of Science and Technology (KAIST)</a></div>
					<div style="float:right; text-align:right">December 2020 - May 2021</div><br>
					Research Assistant<br>
					Advisor: <a href="https://chaoningzhang.github.io">Chaoning Zhang</a>,
					<a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ">In So Kweon</a><br>
					Topic: Deep Data Hiding, Adversarial Attack
				</li>
			</ul>
		</div><br>

		<div id="education">
			<h2>üéì Educations</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">
						<b>Ph.D. student</b>, School of Intelligence Science and Technology,
						<b>Peking University</b>
					</div>
					<div style="float:right; text-align:right">2022 - present</div><br>
				</li>
				<li>
					<div style="float:left; text-align:left">
						<b>B.Eng.</b>, College of Computer Science,
						<b>Sichuan University</b>
					</div>
					<div style="float:right; text-align:right">2018 - 2022</div><br>
					<ul>
						<!-- <li>GPA: 3.91/4.0; Average Score: 93.97/100</li> -->
						<li>Comprehensive Ranking: <b>1/171</b></li>
						<li>Member of Honor College (for top 2% undergraduates at Sichuan University)</li>
					</ul>
				</li>
			</ul>
		</div><br>

		<div id="award">
			<h2>üèÜ Honors & Awards</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">Fresh Ph.D. Student Scholarship of WICT,
						Peking University (only 4 winners at WICT per year)</div>
					<div style="float:right; text-align:right;">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Outstanding Bachelor Thesis, Sichuan University</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left;">BaoSteel Scholarship,
						BaoSteel Education Foundation (only 6 winnners in SCU per year, including postgraduates)</div>
					<div style="float:right; text-align:right;">2021</div>
				</li>
				<li>
					<div style="float:left; text-align:left"><b>National Scholarship</b>,
						Ministry of Education (the highest honor scholarship in China)</div>
					<div style="float:right; text-align:right">2021</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Outstanding Graduate, Sichuan University</div>
					<div style="float:right; text-align:right">2021</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Outstanding Student (Cadre), Sichuan University</div>
					<div style="float:right; text-align:right">2021,2020,2019</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Comprehensive Scholarship, Sichuan University</div>
					<div style="float:right; text-align:right">2021, 2020, 2019</div>
				</li>
			</ul>
		</div><br>

		<div id="service">
			<h2>üìù Services</h2>
			<ul>
				<li>
					Conference Reviewer:
					<ul>
						<li>
							Computer Vision: ICCV 2023-2025, CVPR 2025
						</li>
						<li>
							Machine Learning: NeurIPS 2023-2025, ICLR 2024-2025, ICML 2024-2025
						</li>
						<li>
							Graphics: SIGGRAPH Asia 2025
						</li>
					</ul>
				</li>
				<li>
					Journal Reviewer: TMLR, TPAMI, TMM
				</li>
			</ul>
		</div><br>

		<div id="footer">
			<div id="footer-text"></div>
		</div>

		<p style="text-align:center">
			¬© Chenguo Lin
			<script type="text/javascript" language="javascript">
				if (Date.parse(document.lastModified) != 0) {
					document.write(" | Last updated: " + document.lastModified.substr(0, 10));
				}
			</script>
		</p>

	</div>

</body>

</html>
